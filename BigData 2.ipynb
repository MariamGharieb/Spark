{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7YI7VeuG1md",
        "outputId": "007b6220-33ef-47a9-85cf-be2e5d0f8e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=1c35f51b453529f046bab9fc58cc34aa0f133bc788e743b7dbc95e3188ed0e7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "# All import and installation\n",
        "!pip install pyspark\n",
        "import pyspark\n",
        "from datetime import datetime\n",
        "sc = pyspark.SparkContext('local[*]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "T6ethFDnG4lX",
        "outputId": "7c0b2f1e-7be1-41dc-890f-6adc2bec0a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,049 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,240 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,699 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,344 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,175 kB]\n",
            "Fetched 10.8 MB in 7s (1,503 kB/s)\n",
            "Reading package lists... Done\n",
            "tar: spark-2.3.1-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "combine.txt  pagecounts-20160101-000000_parsed.out  sample_data\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6c2deda33c85:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb4ee135c00>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://a...content-available-to-author-only...e.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        " \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        " \n",
        "!ls\n",
        " \n",
        "import findspark\n",
        "findspark.init()\n",
        " \n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() \n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTsi2j0gG78N",
        "outputId": "c19401e9-37e5-40e9-fb06-c3acc19db086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['aa 271_a.C 1 4675',\n",
              " 'aa Category:User_th 1 4770',\n",
              " 'aa Chiron_Elias_Krase 1 4694',\n",
              " 'aa Dassault_rafaele 2 9372',\n",
              " 'aa E.Desv 1 4662',\n",
              " 'aa File:Wiktionary-logo-en.png 1 10752',\n",
              " 'aa Indonesian_Wikipedia 1 4679',\n",
              " 'aa Main_Page 5 266946',\n",
              " 'aa Requests_for_new_languages/Wikipedia_Banyumasan 1 4733',\n",
              " 'aa Special:Contributions/203.144.160.245 1 5812']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Retrive the first 10 records\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "data.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKQoLXy2G-jV"
      },
      "outputs": [],
      "source": [
        "#Q1-->max for page size\n",
        "\n",
        "def parse(line):\n",
        "  col=line.split(\" \")\n",
        "  pagesize=float(col[3])\n",
        "  key=col[0]\n",
        "  return(key,pagesize)\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "pagesize=data.map(parse)\n",
        "maxx=pagesize.reduceByKey(lambda x , y: max(x,y))\n",
        "maxx.saveAsTextFile('max.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHoQ8i8SHCYr"
      },
      "outputs": [],
      "source": [
        "#Q1-->min for page size\n",
        "\n",
        "def parse_linee(line):\n",
        "  col=line.split(\" \")\n",
        "  pagesize=float(col[3])\n",
        "  key=col[0]\n",
        "  return (key,pagesize)\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "pagesize=data.map(parse_linee)\n",
        "minn=pagesize.reduceByKey(lambda x , y: min(x,y))\n",
        "minn.saveAsTextFile('min.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl2xEM40HFLa"
      },
      "outputs": [],
      "source": [
        "#Q1-->avg for page size\n",
        "\n",
        "def parsee(line):\n",
        "  col=line.split(\" \")\n",
        "  page_size=float(col[3])\n",
        "  return (page_size)\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "page_size=data.map(parsee)\n",
        "listofavg=[]\n",
        "resultt=page_size.mean()\n",
        "listofavg.append(resultt)\n",
        "final_rdd=sc.parallelize(listofavg)\n",
        "final_rdd.saveAsTextFile('avg.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6gte2STHIMI",
        "outputId": "a20db6c0-c6b5-4b34-f687-7e62e359e859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:12.833503\n"
          ]
        }
      ],
      "source": [
        "#Q1 --> not using min,max ,avg\n",
        "# Define the parse function to extract the page size from each line\n",
        "def parse(line):\n",
        "    col = line.split(\" \")\n",
        "    pagesize = float(col[3])\n",
        "    key = col[0]\n",
        "    return (key, pagesize)\n",
        "\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "# Start the timer for map-reduce paradigm\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Map each line to a tuple\n",
        "pagesize = data.map(parse)\n",
        "\n",
        "# Reduce the tuples by key to find the maximum and minimum page sizes\n",
        "max_size = pagesize.reduceByKey(lambda x, y: x if x > y else y)\n",
        "min_size = pagesize.reduceByKey(lambda x, y: x if x < y else y)\n",
        "\n",
        "# Compute the average page size\n",
        "total_size = pagesize.reduceByKey(lambda x, y: x + y).values().sum()\n",
        "num_pages = pagesize.count()\n",
        "avg_size = total_size / num_pages\n",
        "\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "\n",
        "# Save the maximum, minimum, and average page sizes to separate text files\n",
        "max_size.saveAsTextFile(\"max_size.txt\")\n",
        "min_size.saveAsTextFile(\"min_size.txt\")\n",
        "sc.parallelize([avg_size]).saveAsTextFile(\"avg_size.txt\")\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RopjROMRHNv5",
        "outputId": "84c97218-8b0f-4cfa-9dac-234085c0675c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:09.418443\n"
          ]
        }
      ],
      "source": [
        "#َQ2--> number of page title start with The and not en\n",
        "def parse_data(line):\n",
        "  collomns=line.split(\" \")\n",
        "  return(collomns[0],collomns[1])\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "# Start the timer for map-reduce paradigm\n",
        "start_time = datetime.now()\n",
        "\n",
        "page_title=data.map(parse_data)\n",
        "result=page_title.filter(lambda x: (x[1].startswith('The_'))&('en'!= x[0]))\n",
        "pagetitlecount=result.distinct().count()\n",
        "rddd=sc.parallelize([pagetitlecount])\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "rddd.saveAsTextFile('start.txt')\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbSDfAmTHTCk",
        "outputId": "19703bb4-9326-42fc-9009-0967c36db637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:36.622755\n"
          ]
        }
      ],
      "source": [
        "#Q3--> number of unique terms in page title\n",
        "import re\n",
        "def parseLine(line):\n",
        " fields = line.split(\" \")\n",
        " f1=fields[1]\n",
        " s = re.sub(r'[^a-zA-Z_]','', f1).lower()\n",
        " return s\n",
        "\n",
        "def parseLine22(line):\n",
        " fields = line.split(\"_\")\n",
        " return fields\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "pagetitle=data.map(parseLine)# page title\n",
        "page_titlev2=pagetitle.map(parseLine22)\n",
        "unique_terms=page_titlev2.flatMap(lambda x:x).distinct().count()\n",
        "rdd_count = sc.parallelize([unique_terms])\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "rdd_count.saveAsTextFile('unique.txt')#list RDD\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLq0kRg4HkvK",
        "outputId": "684f7e5f-6732-4008-aff1-b1d4c016531d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:00.023023\n"
          ]
        }
      ],
      "source": [
        "#Q4--> title and the number of times it was repeated.\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Map each line to a tuple containing the page title and a count of 1\n",
        "title_counts = data.map(lambda line: (line.split(\" \")[1], 1))\n",
        "\n",
        "# Reduce the tuples by key to count the number of occurrences of each page title\n",
        "title_counts = title_counts.reduceByKey(lambda x, y: x + y)\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "\n",
        "# Save the output to a text file\n",
        "output = \"\\n\".join([f\"{title} {count}\" for title, count in title_counts.collect()])\n",
        "with open(\"repeat.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IKI-w9EH1sv",
        "outputId": "296efb2c-9d25-4ff7-b6a2-f19dc8305dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:00.041940\n"
          ]
        }
      ],
      "source": [
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Map each line to a tuple containing the page title and the rest of the line\n",
        "data = data.map(lambda line: (line.split(\" \")[1], line))\n",
        "\n",
        "# Group the data by page title\n",
        "grouped_data = data.groupByKey()\n",
        "\n",
        "# Combine the data of pages with the same title and print the result\n",
        "result = grouped_data.flatMap(lambda x: [(x[0], [i, j]) for i in x[1] for j in x[1] if i < j])\n",
        "\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "\n",
        "result.saveAsTextFile(\"combine123.txt\")\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR64Jdy7IcqG"
      },
      "source": [
        "# **Sparks Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXSE31vWIhXr",
        "outputId": "986a3dff-de8f-4ccd-aad1-c3345192f8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Sparks loops: 0:00:18.403614\n"
          ]
        }
      ],
      "source": [
        "#Q1\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Convert each line to a tuple containing the page title and size\n",
        "pages = []\n",
        "for line in data.collect():\n",
        "    page_title = line.split(\" \")[1]\n",
        "    page_size = int(line.split(\" \")[3])\n",
        "    pages.append((page_title, page_size))\n",
        "\n",
        "# Compute the min, max, and average page size using loops\n",
        "min_size = float('inf')\n",
        "max_size = float('-inf')\n",
        "total_size = 0\n",
        "num_pages = 0\n",
        "\n",
        "for page in pages:\n",
        "    size = page[1]\n",
        "    if size < min_size:\n",
        "        min_size = size\n",
        "    if size > max_size:\n",
        "        max_size = size\n",
        "    total_size += size\n",
        "    num_pages += 1\n",
        "\n",
        "# Compute the average page size using loops\n",
        "sum_size = 0\n",
        "for page in pages:\n",
        "    sum_size += page[1]\n",
        "avg_size = sum_size / num_pages\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "\n",
        "# Save the output to a text file\n",
        "output = \"Min page size: {}\\nMax page size: {}\\nAvg page size: {}\".format(min_size, max_size, avg_size)\n",
        "with open(\"minmaxavg.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMGpnmRpI7kH",
        "outputId": "1630a038-ac77-4628-ed91-4cc69f914c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Sparks loops: 0:00:16.952353\n"
          ]
        }
      ],
      "source": [
        "#Q2 \n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Initialize counters\n",
        "num_the_pages = 0\n",
        "num_non_en_the_pages = 0\n",
        "\n",
        "# Loop over all the data to count the number of \"The\" pages and non-English \"The\" pages\n",
        "for line in data.collect():\n",
        "    fields = line.split(\" \")\n",
        "    if fields[1].startswith(\"The_\"):\n",
        "        num_the_pages += 1\n",
        "        if not fields[0].startswith(\"en\"):\n",
        "            num_non_en_the_pages += 1\n",
        "end_time = datetime.now()          \n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "\n",
        "# Save the output to a text file\n",
        "output = \"Number of page titles that start with 'The': {}\\nNumber of 'The' pages that are not part of the English project: {}\".format(num_the_pages, num_non_en_the_pages)\n",
        "with open(\"start2.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZL0rig-wPpV",
        "outputId": "09b3d0fd-928b-4bc6-dc54-b457d769de31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Sparks loops: 0:00:27.003999\n"
          ]
        }
      ],
      "source": [
        "#Q3 --> number of unique terms in page titles.\n",
        "from pyspark import SparkContext\n",
        "import re\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "start_time = datetime.now()\n",
        "# Initialize a set to store unique terms\n",
        "unique_terms = set()\n",
        "\n",
        "# Loop over all the data to extract terms and add them to the set\n",
        "for line in data.collect():\n",
        "    # Extract the page title and normalize it\n",
        "    title = line.split(\" \")[1].lower()\n",
        "    title = re.sub(r'[^a-zA-Z0-9_]+', '', title)  # Remove non-alphanumeric characters\n",
        "    terms = title.split(\"_\")\n",
        "    # Add each term to the set of unique terms\n",
        "    for term in terms:\n",
        "        if term:\n",
        "            unique_terms.add(term)\n",
        "\n",
        "# Compute the number of unique terms\n",
        "num_unique_terms = len(unique_terms)\n",
        "\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "# Save the output to a text file\n",
        "output = \"Number of unique terms in page titles: {}\".format(num_unique_terms)\n",
        "with open(\"unique2.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y03y0dtNL8eW",
        "outputId": "05b5335b-c4af-4068-96d7-b9d8ef84369a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Sparks loops: 0:00:17.223693\n"
          ]
        }
      ],
      "source": [
        "#Q4\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "# Initialize a dictionary to store page title counts\n",
        "title_counts = {}\n",
        "\n",
        "# Loop over all the data to count the number of occurrences of each page title\n",
        "for line in data.collect():\n",
        "    title = line.split(\" \")[1]\n",
        "    if title in title_counts:\n",
        "        title_counts[title] += 1\n",
        "    else:\n",
        "        title_counts[title] = 1\n",
        "\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "# Save the output to a text file\n",
        "output = \"\\n\".join([f\"{title} {count}\" for title, count in title_counts.items()])\n",
        "with open(\"repeat2.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpQMwj_YMLWj",
        "outputId": "94c9451a-92bf-4c4d-fc34-33712c9cb402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time - Sparks loops: 0:00:29.865477\n"
          ]
        }
      ],
      "source": [
        "#Q5\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Initialize a dictionary to store page data\n",
        "page_data = {}\n",
        "\n",
        "# Loop over all the data to combine the data of pages with the same title\n",
        "for line in data.collect():\n",
        "    # Extract the page title from the line\n",
        "    title = line.split(\" \")[1]\n",
        "    # Add the line to the list associated with the title key in the dictionary\n",
        "    if title in page_data:\n",
        "        page_data[title].append(line)\n",
        "    else:\n",
        "        page_data[title] = [line]\n",
        "\n",
        "# Loop over all the groups of pages with the same title to create pairs of pages to be displayed\n",
        "page_pairs = []\n",
        "for title, data in page_data.items():\n",
        "    # Loop over all possible pairs of lines within the group\n",
        "    for i in range(len(data)):\n",
        "        for j in range(i+1, len(data)):\n",
        "            # Add each pair of lines to a list\n",
        "            page_pairs.append((title, [data[i], data[j]]))\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "# Save the output to a text file\n",
        "with open(\"combine1.txt\", \"w\") as f:\n",
        "    for pair in page_pairs:\n",
        "        f.write(f\"{pair}\\n\")\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE9nAzmzTb3v",
        "outputId": "cfdcb5f0-7f1f-4609-8643-b4928d3910b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN2Ivpn-GhlC"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/MyFolder\n",
        "!cp -r /content/* /content/drive/MyDrive/MyFolder/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
